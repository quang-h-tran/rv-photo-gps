{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17f3679-d13a-4202-960d-e50473357915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "import corner\n",
    "import pickle\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61bd47e-6ca7-4a6e-b8aa-fd7b3522c547",
   "metadata": {},
   "source": [
    "### Definition of the SquaredExp kernel and its first + second derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4963a1be-f6e0-43ec-8204-d645ca0ec098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_exp(t1, t2, *, sigma=1.0, lnl=np.log(2.5)):\n",
    "    l = np.exp(lnl)\n",
    "    x = (1 / (2 * l**2)) * (t1 - t2)**2\n",
    "    return sigma**2 * np.exp(-x)\n",
    "\n",
    "def sq_exp_cross(t1, t2, *, sigma=1.0, lnl=np.log(2.5)):\n",
    "    l = np.exp(lnl)\n",
    "    return sigma**2*(t1 - t2)*np.exp((-t1**2/2 + t1*t2 - t2**2/2)/l**2)/l**2\n",
    "\n",
    "def sq_exp_grad(t1, t2, *, sigma=1.0, lnl=np.log(2.5)):\n",
    "    l = np.exp(lnl)\n",
    "    return sigma**2*(l**2 - t1**2 + 2*t1*t2 - t2**2)*np.exp((-t1**2/2 + t1*t2 - t2**2/2)/l**2)/l**4\n",
    "\n",
    "def sample_gp(random, K, size=None):\n",
    "    return random.multivariate_normal(np.zeros(K.shape[0]), K, size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185833b-ac11-4819-8fe8-d658bba966b1",
   "metadata": {},
   "source": [
    "### Definition of the diagonal and off-diagonal terms for the covariance block matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7440b2bf-784a-4cda-bad3-cbc29c16765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_11(t_1, t_2, p):\n",
    "    if isinstance(t_1, np.ndarray) and isinstance(t_2, np.ndarray):\n",
    "        t_1 = t_1[:, None]\n",
    "        t_2 = t_2[None, :]\n",
    "        \n",
    "    A, B, C, D, lnl = p\n",
    "    \n",
    "    first_term = A**2 * sq_exp(t_1, t_2, lnl=lnl)\n",
    "    # Note that the 2nd and 3rd terms cancel by a sign\n",
    "    fourth_term = B**2 * sq_exp_grad(t_1, t_2, lnl=lnl)\n",
    "    \n",
    "    return first_term + fourth_term\n",
    "    \n",
    "def K_12(t_1, t_2, p):\n",
    "    \n",
    "    if isinstance(t_1, np.ndarray) and isinstance(t_2, np.ndarray):\n",
    "        t_1 = t_1[:, None]\n",
    "        t_2 = t_2[None, :]\n",
    "    \n",
    "    A, B, C, D, lnl = p\n",
    "    \n",
    "    first_term = A*C * sq_exp(t_1, t_2, lnl=lnl)\n",
    "    second_term = A*D * sq_exp_cross(t_1, t_2, lnl=lnl)\n",
    "    third_term = B*C * sq_exp_cross(t_2, t_1, lnl=lnl)\n",
    "    fourth_term = B*D * sq_exp_grad(t_1, t_2, lnl=lnl)\n",
    "    \n",
    "    return first_term + second_term + third_term + fourth_term\n",
    "\n",
    "def K_21(t_1, t_2, p):\n",
    "    \n",
    "    if isinstance(t_1, np.ndarray) and isinstance(t_2, np.ndarray):\n",
    "        t_1 = t_1[:, None]\n",
    "        t_2 = t_2[None, :]\n",
    "    \n",
    "    A, B, C, D, lnl = p\n",
    "    \n",
    "    first_term = C*A * sq_exp(t_1, t_2, lnl=lnl)\n",
    "    second_term = C*B * sq_exp_cross(t_1, t_2, lnl=lnl)\n",
    "    third_term = D*A * sq_exp_cross(t_2, t_1, lnl=lnl)\n",
    "    fourth_term = D*B * sq_exp_grad(t_1, t_2, lnl=lnl)\n",
    "    \n",
    "    return first_term + second_term + third_term + fourth_term\n",
    "    \n",
    "def K_22(t_1, t_2, p):\n",
    "    \n",
    "    if isinstance(t_1, np.ndarray) and isinstance(t_2, np.ndarray):\n",
    "        t_1 = t_1[:, None]\n",
    "        t_2 = t_2[None, :]\n",
    "    \n",
    "    A, B, C, D, lnl = p\n",
    "    \n",
    "    first_term = C**2 * sq_exp(t_1, t_2, lnl=lnl)\n",
    "    # Note the 2nd and 3rd terms cancel by a sign\n",
    "    fourth_term = D**2 * sq_exp_grad(t_1, t_2, lnl=lnl)\n",
    "    \n",
    "    return first_term + fourth_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4daffc-6ee0-4488-8aa7-ad46ecd3fde5",
   "metadata": {},
   "source": [
    "### Load in time series data, use only certain sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2cdb72-2404-4de2-b7ec-38f0ec55f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_path = \"example data/tau_0.050\"\n",
    "\n",
    "t_flux = np.load(var_path + \"_t.npy\")[500:2500]\n",
    "flux = np.load(var_path + \"_f.npy\")[500:2500]\n",
    "flux_err = np.load(var_path + \"_ferr.npy\")[500:2500]\n",
    "\n",
    "mu = np.mean(flux)\n",
    "flux = (flux / mu - 1) * 1e3\n",
    "flux_err = flux_err * 1e3 / mu\n",
    "\n",
    "t_rad_full = np.load(var_path + \"_t.npy\")[700:1750]\n",
    "rv_full = np.load(var_path + \"_rv.npy\")[700:1750]\n",
    "rv_err_full = np.load(var_path + \"_rverr.npy\")[700:1750]\n",
    "\n",
    "# Pull a random cadence of N~10,25,50\n",
    "rng_seed = 100 #404\n",
    "n_rv_cadence = 20\n",
    "\n",
    "random = np.random.default_rng(rng_seed)\n",
    "inds = np.sort(random.choice(np.arange(len(t_rad_full)), size=n_rv_cadence, replace=False))\n",
    "\n",
    "#inds = np.arange(0, len(t_rad_full), 225)\n",
    "\n",
    "t_rad = t_rad_full[inds]\n",
    "rv = rv_full[inds]\n",
    "rv_err = rv_err_full[inds]\n",
    "\n",
    "trained_data_dic = {\"Flux Time\":t_flux, \"Flux\":flux, \"Flux Error\":flux_err,\n",
    "                    \"RV Time\":t_rad_full, \"RV\":rv_full, \"RV Error\":rv_err,\n",
    "                    \"Sampled RV Time\":t_rad, \"Sampled RV\":rv, \"Sampled RV Error\":rv_err,\n",
    "                    \"RNG Seed\":rng_seed, \"N RV\":n_rv_cadence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "967f6954-4183-4325-ad44-4ffe6b7be915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_data(save=True, data_dict_fn=None, data_dict=None):\n",
    "    data_dict_fn = \"Two Latent GPs Tests/\" + data_dict_fn\n",
    "    \n",
    "    if save:\n",
    "        with open(data_dict_fn, 'wb') as handle:\n",
    "            pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(data_dict_fn, 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)\n",
    "            \n",
    "        return data_dict\n",
    "    \n",
    "#time_series_data(data_dict_fn='f500-3000_rv700-1750_nrv{0}_seed{1}.pickle'.format(n_rv_cadence, rng_seed), data_dict=trained_data_dic)\n",
    "#time_series_data(data_dict_fn='f0-1000_rv500-2000_nrv7_equal_spaced.pickle'.format(), data_dict=trained_data_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "816f5b25-15c7-4cf7-8abe-213da396b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_mat(params, t_f, t_rv):\n",
    "    \"\"\"\n",
    "    function to build covariance matrix\n",
    "    \"\"\"\n",
    "    Kappa11 = K_11(t_f, t_f, params)\n",
    "    Kappa12 = K_12(t_f, t_rv, params)\n",
    "    Kappa21 = Kappa12.T\n",
    "    Kappa22 = K_22(t_rv, t_rv, params)\n",
    "\n",
    "    cov = np.concatenate((\n",
    "          np.concatenate((Kappa11, Kappa12), axis=1),\n",
    "          np.concatenate((Kappa21, Kappa22), axis=1),\n",
    "          ), axis=0)\n",
    "    \n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f211c39d-291f-4edc-851e-f4176e52775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like(r, K):\n",
    "    \"\"\"\n",
    "    Pulled from Dan's notebook, updated with Cholesky decomposition\n",
    "    https://github.com/dfm/gp/blob/main/solutions.ipynb\n",
    "    \n",
    "    The multivariate Gaussian ln-likelihood (up to a constant) for the\n",
    "    vector ``r`` given a covariance matrix ``K``.\n",
    "    \n",
    "    :param r: ``(N,)``   The residual vector with ``N`` points.\n",
    "    :param K: ``(N, N)`` The square (``N x N``) covariance matrix.\n",
    "    \n",
    "    :returns lnlike: ``float`` The Gaussian ln-likelihood. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Slow version, factor ~2x slower.\n",
    "    #return -0.5 * (np.dot(r, np.linalg.solve(K, r)) + np.linalg.slogdet(K)[1])\n",
    "\n",
    "    # Cholesky decomposition, faster\n",
    "    # For more info, check out: https://math.stackexchange.com/questions/3158303/using-cholesky-decomposition-to-compute-covariance-matrix-determinant\n",
    "    try:\n",
    "        cho_decomp = scipy.linalg.cho_factor(K)\n",
    "        log_det_cov = 2*np.sum(np.log(np.diag(cho_decomp[0])))\n",
    "        return -0.5 * (np.dot(r, scipy.linalg.cho_solve(cho_decomp, r)) + log_det_cov) #+ (len(r)*np.log(2.*np.pi)))\n",
    "    except np.linalg.LinAlgError:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b97810-7899-4aca-a2e0-449bddf817d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_neg_log_prob(params, t_f, t_rv, y, y_err):\n",
    "    \n",
    "    #mean_flux, mean_rv = params[0], params[1] # do not implement mean for now\n",
    "    #mean = np.concatenate((np.zeros(len(t_f))+mean_flux, np.zeros(len(t_rv))+mean_rv))\n",
    "    jitter = np.exp(params[0])\n",
    "    kernel_params = params[1:]\n",
    "    \n",
    "    # Compute the covariance matrix for the first GP\n",
    "    K1 = cov_mat(kernel_params[:5], t_f, t_rv)\n",
    "    \n",
    "    # Compute the covariance matrix for the second GP\n",
    "    K2 = cov_mat(kernel_params[5:], t_f, t_rv)\n",
    "    \n",
    "    K = K1 + K2\n",
    "    K[np.diag_indices_from(K)] += y_err**2 + jitter\n",
    "    \n",
    "    # Compute the negative log likelihood\n",
    "    return -log_like(y, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee36572-4f37-4bfa-8589-9ea2639c6cc2",
   "metadata": {},
   "source": [
    "### Optimize model parameters and Sample Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9025762-19c4-4935-958a-f0959f42d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_gp_kernel(y):\n",
    "    \n",
    "    p0 = np.array([np.log(1.0**2.), 0.5, -0.4, 0.7, 5.0, np.log(2.3),\n",
    "                   0.5, -0.4, 0.7, 5.0, np.log(2.3)])\n",
    "    \n",
    "    #b = [(np.log(1e-3**2), np.log(1e2**2)),\n",
    "    #     (0.01, 100.0), (-100.0, 100.0), (-250.0, 250.0), (-500.0, 500.0), np.log((0.1, 5)),\n",
    "    #     (0.01, 100.0), (-100.0, 100.0), (-250.0, 250.0), (-500.0, 500.0), np.log((0.1, 5))]\n",
    "    \n",
    "    b = [(np.log(1e-3**2), np.log(1e2**2)),\n",
    "         (0.01, 100.0), (-1.0, 1.0), (-250.0, 250.0), (-250.0, 250.0), np.log((0.5, 5)),\n",
    "         (0.01, 100.0), (-1.0, 1.0), (-250.0, 250.0), (-250.0, 250.0), np.log((0.5, 5))]\n",
    "    \n",
    "    #options={'disp': None, 'maxls': 20, 'iprint': -1, 'gtol': 1e-08, 'eps': 1e-08, 'maxiter': 15000, 'ftol': 1e-09, 'maxcor': 10, 'maxfun': 20000}\n",
    "    #options = {'maxiter':25000}\n",
    "    \n",
    "    #result = scipy.optimize.minimize(gp_neg_log_prob, p0, args=(t_flux, t_rad, y, np.concatenate((flux_err, rv_err))), method='Nelder-Mead', bounds=b, options=options)\n",
    "    #result = scipy.optimize.minimize(gp_neg_log_prob, p0, args=(t_flux, t_rad, y, np.concatenate((flux_err, rv_err))))#, bounds=b, options=options)\n",
    "    \n",
    "    result = scipy.optimize.minimize(gp_neg_log_prob, p0, args=(t_flux, t_rad, y, np.concatenate((flux_err, rv_err))))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b108666e-40d4-43aa-a7af-6fb9cf7707c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: -2552.4412351024075\n",
      " hess_inv: array([[ 1.81458864e-02,  1.13068135e-03, -2.40525297e-03,\n",
      "         1.96437171e-01,  8.32688241e-02,  1.27067694e-02,\n",
      "        -7.98456834e-03,  4.62868616e-02,  1.60997290e+00,\n",
      "         1.15649425e+00,  9.60654674e-03],\n",
      "       [ 1.13068135e-03,  1.13025247e-03, -4.41307520e-04,\n",
      "         3.63905771e-03, -2.54498581e-02,  3.69234662e-03,\n",
      "        -4.20891371e-03,  1.86990638e-02,  4.25050886e-01,\n",
      "         4.25978732e-01,  3.39465682e-03],\n",
      "       [-2.40525297e-03, -4.41307520e-04,  1.38747903e-03,\n",
      "        -3.40359613e-03, -4.89815116e-02, -6.53106326e-03,\n",
      "         3.81750719e-03, -2.70643487e-02, -6.39845140e-01,\n",
      "        -5.22434264e-01, -5.28711339e-03],\n",
      "       [ 1.96437171e-01,  3.63905771e-03, -3.40359613e-03,\n",
      "         3.08008880e+01,  6.72087652e+00,  5.61981968e-02,\n",
      "        -1.18166776e-01,  8.39420853e-01,  9.29514294e+00,\n",
      "         1.25353598e+01,  2.21774389e-01],\n",
      "       [ 8.32688241e-02, -2.54498581e-02, -4.89815116e-02,\n",
      "         6.72087652e+00,  1.06540698e+01,  1.52417431e-01,\n",
      "        -1.18622872e-02,  4.66080371e-01,  9.17158886e+00,\n",
      "         1.45453926e+00,  1.03209509e-01],\n",
      "       [ 1.27067694e-02,  3.69234662e-03, -6.53106326e-03,\n",
      "         5.61981968e-02,  1.52417431e-01,  3.50630243e-02,\n",
      "        -2.42426656e-02,  1.50423483e-01,  3.50812948e+00,\n",
      "         3.02034832e+00,  2.86357130e-02],\n",
      "       [-7.98456834e-03, -4.20891371e-03,  3.81750719e-03,\n",
      "        -1.18166776e-01, -1.18622872e-02, -2.42426656e-02,\n",
      "         2.72146138e-02, -1.28096515e-01, -2.81390864e+00,\n",
      "        -2.57152878e+00, -2.53074122e-02],\n",
      "       [ 4.62868616e-02,  1.86990638e-02, -2.70643487e-02,\n",
      "         8.39420853e-01,  4.66080371e-01,  1.50423483e-01,\n",
      "        -1.28096515e-01,  8.15336780e-01,  1.81290377e+01,\n",
      "         1.57964206e+01,  1.58879408e-01],\n",
      "       [ 1.60997290e+00,  4.25050886e-01, -6.39845140e-01,\n",
      "         9.29514294e+00,  9.17158886e+00,  3.50812948e+00,\n",
      "        -2.81390864e+00,  1.81290377e+01,  4.37281620e+02,\n",
      "         3.64360609e+02,  3.53729826e+00],\n",
      "       [ 1.15649425e+00,  4.25978732e-01, -5.22434264e-01,\n",
      "         1.25353598e+01,  1.45453926e+00,  3.02034832e+00,\n",
      "        -2.57152878e+00,  1.57964206e+01,  3.64360609e+02,\n",
      "         3.23419520e+02,  3.08128749e+00],\n",
      "       [ 9.60654674e-03,  3.39465682e-03, -5.28711339e-03,\n",
      "         2.21774389e-01,  1.03209509e-01,  2.86357130e-02,\n",
      "        -2.53074122e-02,  1.58879408e-01,  3.53729826e+00,\n",
      "         3.08128749e+00,  3.23818401e-02]])\n",
      "      jac: array([ 1.36718750e-02, -1.08337402e-02,  5.55419922e-03,  6.10351562e-05,\n",
      "       -2.44140625e-04,  7.93457031e-03, -5.73730469e-03,  1.84631348e-02,\n",
      "       -6.10351562e-05, -2.44140625e-04,  8.66699219e-03])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 2315\n",
      "      nit: 135\n",
      "     njev: 192\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([-1.04439034e+01,  1.52955694e-01, -1.61850595e-02, -2.41407858e+01,\n",
      "       -1.09277338e-01, -6.07655961e-01,  1.98651238e+00,  4.19892735e-01,\n",
      "        3.20617238e+01, -6.46419810e+01,  5.46438726e-01])\n"
     ]
    }
   ],
   "source": [
    "y = np.concatenate((flux, rv))\n",
    "res = minimize_gp_kernel(y)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb021c-33c8-4dd4-92e1-a5059808eebd",
   "metadata": {},
   "source": [
    "### Plot model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d184d8ed-1afb-43a1-b292-4a2869829aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_mat_test(params, t_test_f, t_test_rv, t_train_f, t_train_rv):\n",
    "    \"\"\"\n",
    "    function to build covariance matrix for test data\n",
    "    \"\"\"\n",
    "    \n",
    "    Kappa11 = K_11(t_test_f, t_train_f, params)\n",
    "    Kappa12 = K_12(t_test_f, t_train_rv, params)\n",
    "    Kappa21 = K_21(t_test_rv, t_train_f, params)\n",
    "    Kappa22 = K_22(t_test_rv, t_train_rv, params)\n",
    "            \n",
    "    cov = np.concatenate((\n",
    "          np.concatenate((Kappa11, Kappa12), axis=1),\n",
    "          np.concatenate((Kappa21, Kappa22), axis=1),\n",
    "          ), axis=0)\n",
    "    \n",
    "    return cov\n",
    "\n",
    "def trained_cov(p):\n",
    "    \"\"\"\n",
    "    function to build covariance matrix from optimized model parameters computed from training set \n",
    "    \"\"\"\n",
    "    cov_train1 = cov_mat(p[1:6], t_flux, t_rad)\n",
    "    cov_train2 = cov_mat(p[6:], t_flux, t_rad)\n",
    "    cov_train = cov_train1 + cov_train2\n",
    "\n",
    "    cov_train[np.diag_indices_from(cov_train)] += np.concatenate((flux_err, rv_err))**2 + np.exp(p[0])\n",
    "    \n",
    "    return cov_train\n",
    "\n",
    "def predict(p, n_test=1000):\n",
    "    \n",
    "    cov_train = trained_cov(p)\n",
    "    \n",
    "    factor = (scipy.linalg.cholesky(cov_train, overwrite_a=True, lower=False), False)\n",
    "    alpha  = scipy.linalg.cho_solve(factor, y, overwrite_b=True)\n",
    "    \n",
    "    t_test_flux = np.linspace(min(t_flux), max(t_flux), n_test)\n",
    "    t_test_rv  = np.linspace(min(t_rad_full), max(t_rad_full), n_test)\n",
    "    \n",
    "    #k_test_flux = np.zeros((len(t_test_flux)))\n",
    "    #k_test_rv = np.zeros((len(t_test_rv)))\n",
    "    \n",
    "    cov_test_only1 = cov_mat(p[1:6], t_test_flux, t_test_rv)\n",
    "    cov_test_only2 = cov_mat(p[6:],  t_test_flux, t_test_rv)\n",
    "    cov_test_only = cov_test_only1 + cov_test_only2\n",
    "    \n",
    "    cov_test1 = cov_mat_test(p[1:6], t_test_flux, t_test_rv, t_flux, t_rad) # t_flux and t_rad are training data\n",
    "    cov_test2 = cov_mat_test(p[6:],  t_test_flux, t_test_rv, t_flux, t_rad)\n",
    "    cov_test = cov_test1 + cov_test2\n",
    "    \n",
    "    mu = np.dot(cov_test, alpha)\n",
    "    var = cov_test_only[np.diag_indices_from(cov_test_only)]\n",
    "    inv_cov_test = np.linalg.solve(cov_train, cov_test.T)\n",
    "    var -= np.sum(cov_test.T * inv_cov_test, axis = 0)\n",
    "    \n",
    "    return mu, var, t_test_flux, t_test_rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b26912-c300-40a0-ba4a-efa4428da51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var, t_test_flux, t_test_rv = predict(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec823a9c-591e-47d5-ac51-e2e70669e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.04904629 -4.07630418 -4.09417736 ... -9.37080224 -7.98634223\n",
      " -6.58358691]\n"
     ]
    }
   ],
   "source": [
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "545c3def-6935-417f-bf68-a28393f91831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a474537f21e41d7b021a77917a12442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/slx5z51s7j9d_w8l_mb2blvc0001sl/T/ipykernel_20974/3504204243.py:44: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "def plot_data_and_model(p0=None):\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, figsize=(12, 6), gridspec_kw={'hspace':0.4})\n",
    "    \n",
    "    ax[0].scatter(t_flux, flux, color='black', s=5.0, label='LC w/ errors')#, alpha=0.5)\n",
    "    ax[0].plot(t_flux, flux-flux_err, color='grey', label='True LC', zorder=9)#, alpha=0.5)\n",
    "    \n",
    "    rv_train = ax[1].scatter(t_rad, rv, color='orange', s=100.0, alpha=0.9, zorder=10, marker='*', label='Trained RVs')\n",
    "    rv_data = ax[1].scatter(t_rad_full, rv_full, color='black', s=5.0, label='RVs w/ errors')#, alpha=0.3)\n",
    "    true_rv, = ax[1].plot(t_rad_full, rv_full-rv_err_full, color='grey', label='True RVs')#, alpha=0.3)\n",
    "    \n",
    "    ax[0].plot(t_test_flux, mu[:len(t_test_flux)], lw=2.0, color='C0', ls='-', label='GP$_\\mathrm{LC}$', zorder=10)\n",
    "    rv_gp, = ax[1].plot(t_test_rv, mu[len(t_test_flux):], lw=2.0, color='C0', ls='-', zorder=1, label='GP$_\\mathrm{RV}$')\n",
    "    \n",
    "    ax[0].fill_between(t_test_flux, y1=mu[:len(t_test_flux)]-np.sqrt(var[:len(t_test_flux)]),\n",
    "                       y2=mu[:len(t_test_flux)]+np.sqrt(var[:len(t_test_flux)]), color='C0', alpha=0.5)\n",
    "    \n",
    "    ax[1].fill_between(t_test_rv, y1=mu[len(t_test_flux):]-np.sqrt(var[len(t_test_flux):]),\n",
    "                       y2=mu[len(t_test_flux):]+np.sqrt(var[len(t_test_flux):]), color='C0', alpha=0.25)\n",
    "    \n",
    "    ax[0].set_ylabel(r\"Norm. Flux (ppt)\")\n",
    "    ax[1].set_ylabel(r\"RV (m s$^{-1}$)\")\n",
    "    \n",
    "    ax[0].set_xlabel(r\"Time (day)\")\n",
    "    ax[1].set_xlabel(r\"Time (day)\")\n",
    "    \n",
    "    ax[0].set_xlim([min(t_flux), max(t_flux)])\n",
    "    ax[1].set_xlim([min(t_rad), max(t_rad)])\n",
    "    \n",
    "    ax[0].set_ylim([min(flux)-2.0, max(flux)+2.5])\n",
    "    ax[1].set_ylim([min(rv_full)-175., max(rv_full)+150.])\n",
    "    \n",
    "    ax[0].legend(fontsize=12, markerscale=2.0)\n",
    "    rv_l1 = ax[1].legend([rv_train, rv_data], [\"Trained RVs\", \"RVs w/ errors\"], fontsize=12, markerscale=1.5, loc='lower right')\n",
    "    rv_l2 = ax[1].legend([rv_gp, true_rv], [\"GP$_\\mathrm{RV}$\", \"True RV\"], fontsize=12, markerscale=1.5, loc='lower left')\n",
    "    ax[1].add_artist(rv_l1)\n",
    "\n",
    "    data_dict_fn = \"Two Latent GPs Tests/SqExp/\"\n",
    "    plot_fn = data_dict_fn + 'f500-2500_rv700-1750_nrv{0}_seed{1}.png'.format(n_rv_cadence, rng_seed)\n",
    "    #plot_fn = data_dict_fn + 'f0-1000_rv2500-3500_nrv7_equal_spaced.png'\n",
    "    #plt.savefig(plot_fn, bbox_inches='tight', dpi=400)\n",
    "    #plt.savefig(\"Plots/two_latent_GP_model_comp_nRV_10.png\", bbox_inches='tight', dpi=400)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "#plot_data_and_model(p0 = np.array([0.1, 0.04, 0.07, 5.0, 20.0, 2.0]))\n",
    "plot_data_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d923f729-c07f-4d2f-aa5b-7ad86e54d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa9410-af03-44c9-9986-6f96061c408c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
